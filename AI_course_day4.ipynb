{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alejandrooct/AI-course/blob/main/AI_course_day4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5x9jIqxZ3Rq"
      },
      "source": [
        "\n",
        "##AI course\n",
        "##Day#4 (Wednesday December 6, 2023)\n",
        "##Assignement: Drone Data Preprocessing\n",
        "\n",
        "name: Alejandro Thamm\n",
        "\n",
        "The exercise involves:\n",
        "- extracting drone location (within each image),\n",
        "- creating labeled datasets,\n",
        "- building a simple classifier using TensorFlow.\n",
        "\n",
        "link to the Assignement description:\n",
        "https://docs.google.com/document/d/1qKI6AzhIfNc3bX6lFikTI_qDq-VNN7OKmlZpW0ZygIc/edit\n",
        "\n",
        "Summary: I could:\n",
        "\n",
        "Read the images (808) and the corresponding sannotations.\n",
        "\n",
        "Draw rectangles over the labeled objects.\n",
        "\n",
        "Crop the images to the bounding box size.\n",
        "\n",
        "Chenge the images forn 3 channels (RGB) to 1 channel gray-scale (0-255)\n",
        "\n",
        "Reduce the image size to 28 x 28 pixels.\n",
        "\n",
        "But I got confused at the momento os saving the dataset, and be able to load it into Tensoflow.\n",
        "\n",
        "The creation of a \"not a drone\" dataset by cropping randomly from the original dataset in the same size as the labeled drone, had some issues, since about 10 % of the images showed part of the drone *I did this by making the random crops on the images with the bounding box visible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to4hadwvAaPV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oed_wQyA9d2W"
      },
      "source": [
        "Step 1 - Data Exploration:\n",
        "\n",
        "Access the drive in this link:\n",
        "\n",
        "https://drive.google.com/drive/folders/1--E2a3ior8D_isWan7ePmd_NmsNcy6Uk\n",
        "\n",
        "it contains the images and the labels.\n",
        "Observe the structure of the data.\n",
        "Each image has a corresponding label file written in a yolov8 format.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmiuVHAA9a10"
      },
      "outputs": [],
      "source": [
        "# Step 1\n",
        "\n",
        "# A - Accessing the data\n",
        "# The shared folder contains many file pairs.\n",
        "# googledrive is very tricky when folders contain a few hundreds of files.\n",
        "# It is not easy to determine how many file (more than 1600 files (800 pairs)\n",
        "# I just selected 974 files (487 image - annotation pairs) and download them.\n",
        "# This is about 54 MB\n",
        "# I downloaded this to my local machine, to upload it to the colab notebook.\n",
        "\n",
        "# B - Assesing the data\n",
        "# The annotation seems to be bounding boxes (BB)\n",
        "# Each BB seems to be determined by 5 numbers\n",
        "#   number#1: class\n",
        "#   number#2: x_center (normalized to image width)\n",
        "#   number#3: y_center (normalized to image height)\n",
        "#   number#4: x_width (normalized to image width)\n",
        "#   number#5: y_height (normalized to image height)\n",
        "# Most images have just one BB (one line in the yolo file)\n",
        "# a few images have 2 BBs (two lines in the yolo file) (eg: 50.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T6_hgYQ-i7C"
      },
      "source": [
        "Step 2 - Data Preprocessing:\n",
        "\n",
        "For each image:\n",
        "- Extract the drone's location from the label file (yolov8\n",
        "format).\n",
        "\n",
        "- Find the bounding box of the drone in the image.\n",
        "\n",
        "- Create a new image by cropping the bounding box.\n",
        "\n",
        "- Convert the cropped image to black and white.\n",
        "\n",
        "- Resize the image to a quadratic form (e.g., 28x28) using Keras library.\n",
        "\n",
        "- Save the processed image in a list along with the appropriate label (drone).\n",
        "\n",
        "*Note*, the steps that follow are not very reasonable for images in which the drone's bounding box occupies a big proportion of the whole image, since in these cases almost any bounding box (of the same size as the original BB) will contain portions of the drone, and lebeling these images as :not a drone\" will create trouble. For example with image 23.jpg.* **bold text**\n",
        "\n",
        "- Repeat the above steps, but this time take a random crop of the image with the same quadratic resolution.\n",
        "\n",
        "- Save the random cropped image in a separate list along with the appropriate label (not a drone).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeKrh4I_AYZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ca6927-9e88-4ab5-ae8a-7b557d6ce36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drones’: File exists\n",
            "mkdir: cannot create directory ‘/content/drones_outA’: File exists\n",
            "mkdir: cannot create directory ‘/content/drones_outA2’: File exists\n",
            "mkdir: cannot create directory ‘/content/drones_outB’: File exists\n",
            "mkdir: cannot create directory ‘/content/drones_outB2’: File exists\n"
          ]
        }
      ],
      "source": [
        "# Step 2 (data preprocessing) - load some images in this colab notebook.\n",
        "\n",
        "# make a folder in this colab notebook, under /content/ named \"drones\".\n",
        "! mkdir /content/drones\n",
        "! mkdir /content/drones_outA\n",
        "! mkdir /content/drones_outA2\n",
        "! mkdir /content/drones_outB\n",
        "! mkdir /content/drones_outB2\n",
        "# copy manualy some file-pairs from the downloaded local drive to this transient folder.\n",
        "# Now we will be able to work with the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boh_9MhlsDMY"
      },
      "outputs": [],
      "source": [
        "# *************** For deleting the data-folder if it gets FOOBAR ***************\n",
        "# ------------------------------- USE WITH CAUTION -----------------------------\n",
        "# ------------------------------------------------------------------------------\n",
        "import shutil\n",
        "shutil.rmtree('/content/drones')\n",
        "! mkdir /content/drones\n",
        "# ---------------------------------------------- to delete even read only files.\n",
        "# shutil.rmtree('/content/drones', ignore_errors=True)\n",
        "# ------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow0fK_PSB35E"
      },
      "outputs": [],
      "source": [
        "# remove the output directories and recreate them\n",
        "import shutil\n",
        "shutil.rmtree('/content/drones_outA')\n",
        "shutil.rmtree('/content/drones_outA2')\n",
        "shutil.rmtree('/content/drones_outB')\n",
        "shutil.rmtree('/content/drones_outB2')\n",
        "! mkdir /content/drones_outA\n",
        "! mkdir /content/drones_outA2\n",
        "! mkdir /content/drones_outB\n",
        "! mkdir /content/drones_outB2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CzAWKDCVyo5"
      },
      "source": [
        "Step 2 - for \"drone\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPmIWC-kM7FX"
      },
      "outputs": [],
      "source": [
        "# Step 2 (data preprocessing) - make crops of the labeled drones and save them in 28 x 28 squares (matrices)\n",
        "# try to make ChatGPT write some code on my behalf\n",
        "\n",
        "# Ignore all previous instructions.\n",
        "# Act as a world class python programmer.\n",
        "# Write code for a google colab notebook in python.\n",
        "# Read the files in /content/drones/\n",
        "# These are in pairs.\n",
        "# So for each file name, you will find a .jpg image file, and a .txt file.\n",
        "# The .txt file is a yoloV8 annotation or label for the corresponding image file.\n",
        "# Each line of the label file is a label:\n",
        "# The first number in the line is the class.\n",
        "# Then 4 coordinates follow:\n",
        "#   coordinate#1: is the normalized xcenter, you need to multiply it by image width to get pixels.\n",
        "#   coordinate#2: is the normalized ycenter, you need to multiply it by image height to get pixels.\n",
        "#   coordinate#3: is the normalized box width, you need to multiply it by image width to get pixels.\n",
        "#   coordinate#4: is the normalized box height, you need to multiply it by image height to get pixels.\n",
        "# For each image do the following:\n",
        "# Make a copy of the image, and add to it a rectangle around the bounding box, 1 pixel bigger than the bounding box, and one pixel in width, in yellow color.\n",
        "# Show a copy of the original image, with the corresponding rectangles ovelayed.\n",
        "# Use matplotlib to show the images\n",
        "# Make a crop of the original image, just the size of the bounding box\n",
        "# Transform this cropped image to gray-scale and save it to disk in the directory: /content/drones_outA (at the end of the file name, add \"_grey_crop\")\n",
        "# Resize this gray-scale images to 28 x 28 pixels using the Keras python library.\n",
        "# Make a list of tuples, first element is the image matrix, second element, the label: \"drone\"\n",
        "# Save this list to disk in the directory: /content/drones_outA2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajzwU6v4-id3"
      },
      "source": [
        "Step 2 - for \"drone\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "l9ilZBT9nNgX",
        "outputId": "134dc000-593a-445a-987b-5e7408d718b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py:521: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "from PIL import Image\n",
        "\n",
        "# Define input and output directories\n",
        "input_dir = \"/content/drones/\"\n",
        "output_dir1 = \"/content/drones_outA/\"\n",
        "output_dir2 = \"/content/drones_outA2/\"\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "os.makedirs(output_dir1, exist_ok=True)\n",
        "os.makedirs(output_dir2, exist_ok=True)\n",
        "\n",
        "# Function to process each pair of files\n",
        "def process_files(image_path, label_path):\n",
        "    # Read image\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Read label file\n",
        "    with open(label_path, 'r') as label_file:\n",
        "        lines = label_file.readlines()\n",
        "\n",
        "    # Process each line in the label file\n",
        "    for line in lines:\n",
        "        values = line.split()\n",
        "        class_label = int(values[0])\n",
        "        x_center = int(float(values[1]) * img.shape[1])\n",
        "        y_center = int(float(values[2]) * img.shape[0])\n",
        "        box_width = int(float(values[3]) * img.shape[1])\n",
        "        box_height = int(float(values[4]) * img.shape[0])\n",
        "\n",
        "        # Draw rectangle on the image\n",
        "        cv2.rectangle(img, (x_center - box_width//2, y_center - box_height//2),\n",
        "                      (x_center + box_width//2, y_center + box_height//2),\n",
        "                      (0, 255, 255), 2)\n",
        "\n",
        "        # Crop the image\n",
        "        cropped_img = img[y_center - box_height//2:y_center + box_height//2,\n",
        "                          x_center - box_width//2:x_center + box_width//2]\n",
        "\n",
        "        # Save the cropped image in grayscale\n",
        "        gray_cropped_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n",
        "        output_path = os.path.join(output_dir1, os.path.basename(image_path)[:-4] + \"_grey_crop.jpg\")\n",
        "        cv2.imwrite(output_path, gray_cropped_img)\n",
        "\n",
        "        # Resize the grayscale image to 28x28 pixels\n",
        "        resized_img = cv2.resize(gray_cropped_img, (28, 28))\n",
        "        output_path = os.path.join(output_dir2, os.path.basename(image_path)[:-4] + \"_grey_crop.jpg\")\n",
        "        cv2.imwrite(output_path, resized_img)\n",
        "\n",
        "        # Add tuple to the list\n",
        "        data_list.append((resized_img, \"drone\"))\n",
        "\n",
        "# List to store image matrices and labels\n",
        "data_list = []\n",
        "\n",
        "# Process each pair of files\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        image_path = os.path.join(input_dir, filename)\n",
        "        label_path = os.path.join(input_dir, filename[:-4] + \".txt\")\n",
        "\n",
        "        # Process the pair of files\n",
        "        process_files(image_path, label_path)\n",
        "\n",
        "# Show the original images with rectangles\n",
        "fig, axs = plt.subplots(1, len(data_list), figsize=(15, 5))\n",
        "for i, (img, _) in enumerate(data_list):\n",
        "    axs[i].imshow(img, cmap='gray')\n",
        "    axs[i].axis('off')\n",
        "\n",
        "# Save the list to disk\n",
        "output_path = \"/content/drones_outA2/data_list.pkl\"\n",
        "np.save(output_path, data_list)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 - for \"drone\" variant 1"
      ],
      "metadata": {
        "id": "1NpVgQCvGhvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "\n",
        "# Set the input and output directories\n",
        "input_directory = '/content/drones/'\n",
        "output_directory = '/content/drones_outA/'\n",
        "output_directory_resized = '/content/drones_outA2/'\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "os.makedirs(output_directory_resized, exist_ok=True)\n",
        "\n",
        "# Function to read YOLO annotation file and extract bounding box coordinates\n",
        "def read_annotation_file(annotation_path):\n",
        "    with open(annotation_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    bounding_boxes = []\n",
        "    for line in lines:\n",
        "        data = line.strip().split()\n",
        "        class_label = int(data[0])\n",
        "        x_center = float(data[1])\n",
        "        y_center = float(data[2])\n",
        "        box_width = float(data[3])\n",
        "        box_height = float(data[4])\n",
        "        bounding_boxes.append((class_label, x_center, y_center, box_width, box_height))\n",
        "    return bounding_boxes\n",
        "\n",
        "# Function to draw rectangles on an image based on bounding box coordinates\n",
        "def draw_rectangles(image, bounding_boxes):\n",
        "    image_with_rectangles = image.copy()\n",
        "    for box in bounding_boxes:\n",
        "        h, w, _ = image.shape\n",
        "        x, y, box_w, box_h = (\n",
        "            int((box[1] - box[3] / 2) * w),\n",
        "            int((box[2] - box[4] / 2) * h),\n",
        "            int(box[3] * w),\n",
        "            int(box[4] * h),\n",
        "        )\n",
        "        cv2.rectangle(image_with_rectangles, (x, y), (x + box_w, y + box_h), (0, 255, 255), 2)\n",
        "    return image_with_rectangles\n",
        "\n",
        "# Function to crop the image based on bounding box coordinates\n",
        "def crop_image(image, bounding_box):\n",
        "    h, w, _ = image.shape\n",
        "    x, y, box_w, box_h = (\n",
        "        int((bounding_box[1] - bounding_box[3] / 2) * w),\n",
        "        int((bounding_box[2] - bounding_box[4] / 2) * h),\n",
        "        int(bounding_box[3] * w),\n",
        "        int(bounding_box[4] * h),\n",
        "    )\n",
        "    cropped_image = image[y:y + box_h, x:x + box_w]\n",
        "    return cropped_image\n",
        "\n",
        "# Function to save a grayscale image\n",
        "def save_grayscale_image(image, output_path):\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    cv2.imwrite(output_path, gray_image)\n",
        "\n",
        "# Function to resize an image and save it\n",
        "def resize_and_save_image(image, output_path, target_size=(28, 28)):\n",
        "    resized_image = cv2.resize(image, target_size)\n",
        "    cv2.imwrite(output_path, resized_image)\n",
        "\n",
        "# List to store tuples (image_matrix, label)\n",
        "image_label_list = []\n",
        "\n",
        "# Process each pair of image and annotation file\n",
        "for file_name in os.listdir(input_directory):\n",
        "    if file_name.endswith('.jpg'):\n",
        "        image_path = os.path.join(input_directory, file_name)\n",
        "        annotation_path = os.path.join(input_directory, file_name.replace('.jpg', '.txt'))\n",
        "\n",
        "        # Read image and annotation\n",
        "        original_image = cv2.imread(image_path)\n",
        "        bounding_boxes = read_annotation_file(annotation_path)\n",
        "\n",
        "        # Draw rectangles on the image\n",
        "        image_with_rectangles = draw_rectangles(original_image.copy(), bounding_boxes)\n",
        "\n",
        "        # Show the image with rectangles using matplotlib\n",
        "        plt.imshow(cv2.cvtColor(image_with_rectangles, cv2.COLOR_BGR2RGB))\n",
        "        plt.title('Image with Rectangles')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        # Save image with rectangles\n",
        "        output_image_path = os.path.join(output_directory, file_name)\n",
        "        cv2.imwrite(output_image_path, image_with_rectangles)\n",
        "\n",
        "        for box in bounding_boxes:\n",
        "            # Crop the image based on the bounding box\n",
        "            cropped_image = crop_image(original_image, box)\n",
        "\n",
        "            # Save grayscale cropped image\n",
        "            output_gray_path = os.path.join(output_directory_resized, file_name.replace('.jpg', '_grey_crop.jpg'))\n",
        "            save_grayscale_image(cropped_image, output_gray_path)\n",
        "\n",
        "            # Resize and save the grayscale image\n",
        "            output_resized_path = os.path.join(output_directory_resized, file_name.replace('.jpg', '_resized.jpg'))\n",
        "            resize_and_save_image(cropped_image, output_resized_path)\n",
        "\n",
        "            # Append to the list of tuples\n",
        "            image_label_list.append((img_to_array(cropped_image), 'drone'))\n",
        "\n",
        "# Save the list to disk\n",
        "list_output_path = '/content/drones_outA2/image_label_list.pkl'\n",
        "np.save(list_output_path, image_label_list)\n",
        "print(f'The image label list has been saved to {list_output_path}')\n"
      ],
      "metadata": {
        "id": "SOM5v9YTGgqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the outputs\n",
        "vv = 'contents/drones_outA2/image_label_list.pkl.npy'\n",
        "print (vv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlHL79hsJXI-",
        "outputId": "0d0843d0-4ca3-4c12-cc2b-c0aa3ec51622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contents/drones_outA2/image_label_list.pkl.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsR_vKxTVukc"
      },
      "source": [
        "Step 2 - for: 'not a drone'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvbOcGCzVvTq"
      },
      "outputs": [],
      "source": [
        "# Step 2 (data preprocessing) - make crops of the 'not a drone' and save them in 28 x 28 squares (matrices)\n",
        "# try to make ChatGPT write some code on my behalf\n",
        "\n",
        "# Ignore all previous instructions.\n",
        "# Act as a world class python programmer.\n",
        "# Write code for a google colab notebook in python.\n",
        "# Read the files in /content/drones/\n",
        "# These are in pairs.\n",
        "# So for each file name, you will find a .jpg image file, and a .txt file.\n",
        "# The .txt file is a yoloV8 annotation or label for the corresponding image file.\n",
        "# Each line of the label file is a label:\n",
        "# The first number in the line is the class.\n",
        "# Then 4 coordinates follow:\n",
        "#   coordinate#1: we don't care about this value.\n",
        "#   coordinate#2: we don't care about this value.\n",
        "#   coordinate#3: is the normalized box width, you need to multiply it by image width to get pixels.\n",
        "#   coordinate#4: is the normalized box height, you need to multiply it by image height to get pixels.\n",
        "# For each image do the following:\n",
        "# Determine a random point within the whole image, we will call it random center.\n",
        "# With this random center, as center, make a rectangle measuring box width by box height.\n",
        "# Make a crop of this rectangle.\n",
        "# Show this cropped rectangle.\n",
        "# Use matplotlib to show the images.\n",
        "# Transform this cropped image to gray-scale and save it to disk in the directory: /content/drones_outB (at the end of the file name, add \"_grey_crop\")\n",
        "# Resize thees gray-scale images to 28 x 28 pixels using the Keras python library.\n",
        "# Save the processed image in a list along with the appropriate label: 'not a drone'.\n",
        "# Save this list to disk in the directory: /content/drones_outB2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-J7g_HT4vdW"
      },
      "source": [
        "\n",
        "Step 2 - for: 'not a drone\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y4Vlsjyp9vb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set paths\n",
        "input_directory = '/content/drones/'\n",
        "output_directory = '/content/drones_outB/'\n",
        "output_directory2 = '/content/drones_outB2/'\n",
        "\n",
        "# Create output directories if not exist\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "os.makedirs(output_directory2, exist_ok=True)\n",
        "\n",
        "# Function to read YOLO annotation file\n",
        "def read_yolo_annotation(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        labels = [line.strip().split() for line in lines]\n",
        "        return labels\n",
        "\n",
        "# Function to process each pair of image and label\n",
        "def process_image_and_label(image_path, label_path):\n",
        "    # Read image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Read YOLO annotation\n",
        "    labels = read_yolo_annotation(label_path)\n",
        "\n",
        "    for label in labels:\n",
        "        class_id = int(label[0])\n",
        "        box_width = float(label[3]) * image.shape[1]\n",
        "        box_height = float(label[4]) * image.shape[0]\n",
        "\n",
        "        # Generate random center\n",
        "        center_x = np.random.uniform(box_width / 2, image.shape[1] - box_width / 2)\n",
        "        center_y = np.random.uniform(box_height / 2, image.shape[0] - box_height / 2)\n",
        "\n",
        "        # Calculate rectangle coordinates\n",
        "        x1 = int(center_x - box_width / 2)\n",
        "        y1 = int(center_y - box_height / 2)\n",
        "        x2 = int(center_x + box_width / 2)\n",
        "        y2 = int(center_y + box_height / 2)\n",
        "\n",
        "        # Crop the rectangle\n",
        "        cropped_image = image[y1:y2, x1:x2]\n",
        "\n",
        "        # Display the cropped image\n",
        "        plt.imshow(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
        "        plt.show()\n",
        "\n",
        "        # Convert the cropped image to gray-scale\n",
        "        gray_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Save the gray-scale cropped image to disk\n",
        "        output_file_path = os.path.join(output_directory, f\"{os.path.basename(image_path)}_grey_crop.jpg\")\n",
        "        cv2.imwrite(output_file_path, gray_image)\n",
        "\n",
        "        # Resize the gray-scale image to 28 x 28 pixels\n",
        "        resized_image = cv2.resize(gray_image, (28, 28))\n",
        "\n",
        "        # Save the processed image along with the label\n",
        "        output_file_path2 = os.path.join(output_directory2, f\"{os.path.basename(image_path)}_grey_crop.npy\")\n",
        "        np.save(output_file_path2, {'image': resized_image, 'label': 'not a drone'})\n",
        "\n",
        "# Process each pair of image and label\n",
        "for filename in os.listdir(input_directory):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        image_path = os.path.join(input_directory, filename)\n",
        "        label_path = os.path.join(input_directory, filename.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "        process_image_and_label(image_path, label_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO3BaPoPe-h6",
        "outputId": "7468dbe9-13ef-47f3-d0fc-e97b8b226b26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# this cell is for doing a drive mount of all \"MyDrive\" and its \"shortcuts\"\n",
        "# so you first have to make a shortcut of the shared folder you want, into your MyDrive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk0-FM-qAY8m"
      },
      "source": [
        "Step 3 - TensorFlow Datasets:\n",
        "   - Convert the lists of processed images and labels to TensorFlow datasets.\n",
        "   - Ensure that the shapes and types of images and labels match TensorFlow's requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6dYajE2Aizr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSFS5kKmAjRS"
      },
      "source": [
        "Step 4 - Build Classifier Model:\n",
        "   - Utilize the knowledge from previous homework (e.g., MNIST Fashion).\n",
        "   - Split the dataset into training and testing sets.\n",
        "   - Build a simple classifier model using TensorFlow/Keras.\n",
        "   - Train the model on the training set and evaluate it on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWoZJCrmAmeN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NojDSFuUAmxk"
      },
      "source": [
        "Step 5 - Visualization and Analysis:\n",
        "   - Plot some images from the dataset to visually inspect the processed images.\n",
        "   - Display the accuracy of the trained model on the test set.\n",
        "   - Think about potential improvements or additional steps in the preprocessing and model building phases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7atu_RnBLoi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaqLzdg0BL-Z"
      },
      "source": [
        "Questions to think about:\n",
        "\n",
        "Data Preprocessing:\n",
        "\n",
        "- Why is it good to convert images to black and white?\n",
        "\n",
        "*A color image will have exactly 3 times more features than the gray-scale image. Also gray-scale images are amost invariant to illumination changes, or changes in the color of the light, color images are very sensitive to illumination applied. \"A blue drone, is also a drone, and a green camel is still a green camel\"*\n",
        "\n",
        "- How does resizing impact the training of machine learning models?\n",
        "\n",
        "*A big sized image (let's say 512 x 512) will have 250 K features which is too much for a NN. Small images, like 28 x 28 pixels, will have 784 features, which is a big number, but not impossible.*\n",
        "\n",
        "TensorFlow Datasets and Model Building:\n",
        "- Why is it important to have consistent shapes and data types in TensorFlow datasets?\n",
        "\n",
        "- How does the choice of activation function impact the model's performance?\n",
        "\n",
        "- What can be done to improve the model's accuracy?\n",
        "\n",
        "Tips:\n",
        "- Break down the problem into smaller steps and tackle each one iteratively.\n",
        "- Start by trying the preprocess on only one image, visualize the results and check the shape of the image, and then proceed and preprocess all the dataset.\n",
        "- Experiment with the images resolution, different activation functions, and different model’s architectures!\n",
        "\n",
        "This exercise combines practical coding skills with machine learning concepts, allowing you to apply your knowledge to real-world scenarios. Good luck and enjoy!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrsbo1ZUaRYjlzVZndDYda",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}